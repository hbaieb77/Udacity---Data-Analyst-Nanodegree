{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report will go in details about a data wrangling process performed on some data provided by the \"WeRateDogs\" twitter account "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Gathering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we were asked to gather three essentiel pieces of data\n",
    "\n",
    "**2.1: The WeRateDogs archive dataset**\n",
    "\n",
    "We have manually imported this data from a csv file already downloaded \"twitter-archive-enhanced.csv\" .     \n",
    "\n",
    "This dataset gives us basic information about each tweet's text , source , time posted etc etc ... \n",
    "\n",
    "**2.2: The Tweet Image predictions dataset**\n",
    "\n",
    "This data was hosted on Udacity's server , we downloaded it using the Requests library and stored it in a file in .tsv format\n",
    "\n",
    "This data contains some dog images predictions provided by their model , in which each prediction had confidance coefficient   \n",
    "\n",
    "**2.3: Additional data from the twitter API**\n",
    "\n",
    "We had the chance to explore and to experiment with the twitter API \"Tweepy\" , it helped us get the retweet and the favorite counts for each tweet . After that , we saved this data in a .txt file later on to be used for future analysis \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting all our data , we then move on to analyse our problem \n",
    "\n",
    "During this phase we were faced with 2 major issues : Quality and tidiness issues "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "\n",
    "\n",
    "##### Archive : \n",
    "\n",
    "- Data Contains retweets and replys \n",
    "\n",
    "- Tweet_id is in int and not string type  \n",
    "\n",
    "- Source column contains HTML syntax  \n",
    "\n",
    "- Name column contains false names and names in lowercase \n",
    "\n",
    "- Rating numerators and denominators have some outliers\n",
    "\n",
    "- Timestamp data is in object and not Datetime type \n",
    "\n",
    "##### Image Predictions : \n",
    "\n",
    "- There are duplicate images\n",
    "\n",
    "- tweet_ID is not in string type \n",
    "\n",
    "##### Json tweets :\n",
    "\n",
    "- tweet_ID is not in string type \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "- Dog stage information is given in 4 columns\n",
    "\n",
    "- Useless IDs and time columns related to retweets and replys  \n",
    "\n",
    "- Useless second and third best model dog predictions\n",
    "\n",
    "- All of these tables should be joined on 'tweet_id' column in order to group all the data in one table so it's easier to analyse "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the hardest and longest part of the project , during this phase I was able to go through all the issues discussed above . I have converted all tweet_id columns to their preferable type , timestamp to Datetime . I have dropped a lot of unwanted columns and cleaned a lot of misstyped data . \n",
    "\n",
    "Of course all of these tasks were done following the code-test process , so I had to make sure all of my changes were applied properly before I moved to other issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "88033ea6e6aaf353f3d26ef69434bb9b1f089d6b00d896155ae24c39a5d92896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
